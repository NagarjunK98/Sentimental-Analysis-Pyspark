{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import Tokenizer,StopWordsRemover\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator,ParamGridBuilder\n",
    "import numpy as np\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark as ps\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "import warnings\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "    try:\n",
    "        sc = ps.SparkContext('local[*]')\n",
    "        sqlContext = SQLContext(sc)\n",
    "        print(\"Just created a SparkContext\")\n",
    "    except ValueError:\n",
    "        warnings.warn(\"SparkContext already exists in this scope\")\n",
    "    spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(fileName):\n",
    "    df=spark.read.csv(fileName,sep=\",\",inferSchema=True,header=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(df):\n",
    "#     df.printSchema()\n",
    "#     df.show(truncate=False)\n",
    "    df.count()\n",
    "    \n",
    "    df1=df.withColumnRenamed('_c0',\"id\").withColumnRenamed('_c1','label').withColumnRenamed('_c2','tweet')\n",
    "#     df1.printSchema()\n",
    "#     df1.show(truncate=False)\n",
    "    \n",
    "    df2 = df1.withColumn('tweet', regexp_replace('tweet', '[^a-z0-9A-Z`~!@#$%&<>?., ]', ''))\n",
    "#     df2.show(truncate=False)\n",
    "    \n",
    "    df3 = df2.withColumn('tweet', regexp_replace('tweet', '[0-9`~!@#$%&<>?,\\']', ''))\n",
    "#     df3.show(truncate=False)\n",
    "    \n",
    "    df4 = df3.withColumn('tweet', regexp_replace('tweet', 'http://*.*.com', ''))\n",
    "#     df4.show(truncate=False)\n",
    "    \n",
    "    df5 = df4.withColumn('tweet', regexp_replace('tweet', 'www.*.com', ''))\n",
    "#     df5.show(truncate=False)\n",
    "    \n",
    "    df6 = df5.withColumn('tweet', regexp_replace('tweet', '\\.', ''))\n",
    "#     df6.show(truncate=False)\n",
    "    \n",
    "    tokenizer=Tokenizer(inputCol=\"tweet\",outputCol=\"words\")\n",
    "    wordData=tokenizer.transform(df6)\n",
    "#     wordData.show()\n",
    "    \n",
    "    remover=StopWordsRemover(inputCol=\"words\",outputCol=\"word_clean\")\n",
    "    word_clean_data=remover.transform(wordData)\n",
    "#     word_clean_data.show()\n",
    "    \n",
    "    stemmer = SnowballStemmer(language='english')\n",
    "    stemmer_udf = udf(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
    "    count=CountVectorizer(inputCol=\"word_clean\",outputCol=\"rawFeatures\")\n",
    "#     print(count)\n",
    "    \n",
    "    model=count.fit(word_clean_data)\n",
    "#     print(model)\n",
    "    \n",
    "    featurizedData=model.transform(word_clean_data)\n",
    "#     featurizedData.show()\n",
    "    idf=IDF(inputCol=\"rawFeatures\",outputCol=\"features\")\n",
    "    idfModel=idf.fit(featurizedData)\n",
    "    rescaledData=idfModel.transform(featurizedData)\n",
    "#     rescaledData.select(\"label\",\"features\").show()\n",
    "    return rescaledData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df):\n",
    "    seed=0\n",
    "    trainDf,testDf=df.randomSplit([0.7,0.3],seed)\n",
    "    trainDf.count()\n",
    "    testDf.count()\n",
    "    return trainDf,testDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic regression(train_data,test_data):\n",
    "    #complete this function\n",
    "    #similarly write functions for other methods\n",
    "    #where train and test data are provided as arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: SparkContext already exists in this scope\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|label|               tweet|               words|          word_clean|         rawFeatures|            features|\n",
      "+---+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  2|    0|user user thanks ...|[user, user, than...|[user, user, than...|(39572,[0,1,19,22...|(39572,[0,1,19,22...|\n",
      "|  3|    1|user that was fuc...|[user, that, was,...|[user, fucking, w...|(39572,[1,525,126...|(39572,[1,525,126...|\n",
      "|  4|    1|userthat was so s...|[userthat, was, s...|  [userthat, shitty]|(39572,[2934,3100...|(39572,[2934,3100...|\n",
      "|  6|    0| huge fan fare an...|[, huge, fan, far...|[, huge, fan, far...|(39572,[0,15,107,...|(39572,[0,15,107,...|\n",
      "|  7|    0| user camping tom...|[, user, camping,...|[, user, camping,...|(39572,[0,1,57,19...|(39572,[0,1,57,19...|\n",
      "+---+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_spark()\n",
    "df=read_file(\"twitter.csv\")\n",
    "df=pre_process(df)\n",
    "train_data,test_data=train_test_split(df)\n",
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
